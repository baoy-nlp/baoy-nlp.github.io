## About Me

This is Yu Bao (È≤çÂÆá). My name in Chinese Pinyin has two interesting extensions, one means rainstorm (üåß) and the other means abalone (üêü). I prefer the former one, as it looks very powerful. Actually, my name comes from "Staying together through thick and thin (fƒìng y«î t√≥ng zh≈çu)" because I also have an older brother named [Feng Bao](https://scholar.google.com/citations?user=U0cuO94AAAAJ&hl=zh-CN).

I am currently a Research Scientist at ByteDance (since April 2022), specializing in Large Language Model (LLM) Alignment and multimodal modeling within the Seed Team. 
Prior to this, I completed my Ph.D. at the [Natural Language Processing Group](http://nlp.nju.edu.cn/homepage/) of [Nanjing University](https://grawww.nju.edu.cn/main.htm), co-supervised by Prof. [Shujian Huang](http://nlp.nju.edu.cn/huangsj/) and Prof. [Jiajun Chen](https://cs.nju.edu.cn/chenjiajun/). During my doctoral studies, I interned twice at ByteDance AI Lab under the mentorship of Prof. [Zhou Hao](https://zhouh.github.io/) and Prof. [Lei Li](https://lileicc.github.io/). I hold a Bachelor's Degree from the School of Science at [Northeast Forestry University](https://www.nefu.edu.cn/).

My work bridges multiple areas:
- **Generative Modeling:** Autoregressive and non-autoregressive frameworks (e.g., diffusion models)
- **Structure-Aware Learning:** Graph-based representations (e.g., molecules) and sequential data modeling
- **Multimodal Alignment:** Transferring structure-sequence joint modeling expertise to text-audio modalities

<!--All of my research interests focus on deep generative models. During my Ph. D. studies and internships at ByteDance AI Lab, I worked on structure and sequence modeling in deep generative models, focusing on machine translation and natural language generation. Now, as a member of ByteDance Research, my focus has turned to AI for Science, especially structure-based drug design.-->

<!--<center><a href="mailto:nlp.baoy@gmail.com">[Email]</a> <a href="./files/baoy_CV.pdf">[CV]</a></center>-->

***We're hiring! The ByteDance Seed Team is actively seeking exceptional talents in LLM. Feel free to contact me and apply via <a href="mailto:baoyu.001@bytedance.com">baoyu.001@bytedance.com</a> for the **Top Seed Internship** program.***


<!-- ### Awards

- 2022, Excellent Doctoral Paper Award, JiangSu Association of Artificial Intelligence.
- 2020, Outstanding Ph.D. Candidate, Nanjing University
- 2019, Artificial Intelligence Scholarship, Nanjing University
- 2019, Outstanding Graduate Student, Nanjing University -->

## Selected Publications/Preprints

<a href="https://scholar.google.com/citations?authuser=1&user=TqMb6nMAAAAJ">[Full Publications]</a> \[*: equal contributions\] \[‚Ä†: interns/students I mentored\]

<!--**<u>AI for Science (Structure-based Drug Design)</u>**

- Xiangxin Zhou\*‚Ä†, Xiwei Cheng\*‚Ä†, Yuwei Yang, **Yu Bao**, Liang Wang, Quanquan Gu, [DecompOpt: Controllable and Decomposed Diffusion Models for Structure-based Molecular Optimization](https://arxiv.org/abs/2403.13829), ICLR 2024.
- Jiaqi Guan\*‚Ä†, Xiangxin Zhou\*‚Ä†, Yuwei Yang, **Yu Bao**, Jian Peng, Jianzhu Ma, Qiang Liu, Liang Wang, Quanquan Gu, [DecompDiff: Diffusion Models with Decomposed Priors for Structure-Based Drug Design](https://arxiv.org/abs/2403.07902), ICML 2023.-->

<!-- **<u>Deep Generative Models</u>** -->
<!-- **Natural Language Processing and Text Generation** -->
1. Shimao Zhang‚Ä†, **Yu Bao**, Shujian Huang, [EDT: Improving Large Language Models by Entropy-based Dynamic Temperature Sampling](https://arxiv.org/pdf/2403.14541.pdf), Preprint 2024.
2. Xiangxin Zhou\*‚Ä†, Xiwei Cheng\*‚Ä†, Yuwei Yang, **Yu Bao**, Liang Wang, Quanquan Gu, [DecompOpt: Controllable and Decomposed Diffusion Models for Structure-based Molecular Optimization](https://arxiv.org/abs/2403.13829), ICLR 2024.
3. Jiaqi Guan\*‚Ä†, Xiangxin Zhou\*‚Ä†, Yuwei Yang, **Yu Bao**, Jian Peng, Jianzhu Ma, Qiang Liu, Liang Wang, Quanquan Gu, [DecompDiff: Diffusion Models with Decomposed Priors for Structure-Based Drug Design](https://arxiv.org/abs/2403.07902), ICML 2023.
4. Min Liu‚Ä†, **Yu Bao**, Chengqi Zhao, Shujian Huang, [Selective Knowledge Distillation for Non-Autoregressive Neural Machine Translation](https://arxiv.org/abs/2303.17910), AAAI 2023.
5. **Yu Bao**, Hao Zhou, Shujian Huang, Dongqi Wang, Lihua Qian, Xinyu Dai, Jiajun Chen, Lei Li, [latent-GLAT: Glancing at Latent Variables for Parallel Text Generation](https://baoy-nlp.github.io/files/Latent_GLAT.pdf), ACL 2022.
6. Lihua Qian, Hao Zhou, **Yu Bao**, Mingxuan Wang, Lin Qiu, Weinan Zhang, Yong Yu, Lei Li, [Glancing Transformer for Non-Autoregressive Neural Machine Translation](https://aclanthology.org/2021.acl-long.155.pdf), ACL 2021.
7. **Yu Bao**, Shujian Huang, Tong Xiao, Dongqi Wang, Xinyu Dai, Jiajun Chen, [Non-Autoregressive Translation by Learning Target Categorical Codes](https://aclanthology.org/2021.naacl-main.458.pdf), NAACL-HLT 2021.
8. Jiahuan Li*, **Yu Bao**\*, Shujian Huang, Xinyu Dai, Jiajun Chen, [Explicit Semantic Decomposition for Definition Generation](https://virtual.acl2020.org/paper_main.65.html), ACL 2020.
9. **Yu Bao**, Hao Zhou, Jiangtao Feng, Mingxuan Wang, Shujian Huang, Jiajun Chen, Lei Li, [PNAT: Non-Autoregressive Transformer by Position Learning](https://arxiv.org/abs/1911.10677), Preprint 2019. 
10. **Yu Bao**\*, Hao Zhou*, Shujian Huang, Lei Li, Lili Mou, Olga Vechtomova, Xinyu Dai, Jiajun Chen, [Generating Sentences from Disentangled Syntactic and Semantic Spaces](https://aclanthology.org/P19-1602.pdf), ACL 2019.

<!-- 6. Shimao Zhang‚Ä†, Yu Bao, Shujian Huang, [EDT: Improving Large Language Models by Entropy-based Dynamic Temperature Sampling](https://arxiv.org/pdf/2403.14541.pdf), Preprint 2024.
7. Jiasheng Ye, Zaixiang Zheng, Yu Bao, Lihua Qian, Mingxuan Wang, [DiNoiSer: Diffused Conditional Sequence Learning by Manipulating Noises](https://arxiv.org/abs/2302.10025), Transaction of ACL (2024).
8. Yu Bao, Shujian Huang, Hao Zhou, Lei Li, Xinyu Dai, Jiajun Chen, [Unsupervised Paraphrasing via Syntactic Template Sampling](https://www.sciengine.com/SSI/doi/10.1360/SSI-2021-0065;JSESSIONID=81ea9517-be4e-4348-81b7-739c29cb09ac), SCIENTIA SINICA Informationis (2022).
9. Jiahuan Li*, Yu Bao*, Shujian Huang, Xinyu Dai, Jiajun Chen, [Explicit Semantic Decomposition for Definition Generation](https://virtual.acl2020.org/paper_main.65.html), ACL 2020.
10. Yu Bao*, Hao Zhou*, Shujian Huang, Lei Li, Lili Mou, Olga Vechtomova, Xinyu Dai, Jiajun Chen, [Generating Sentences from Disentangled Syntactic and Semantic Spaces](https://aclanthology.org/P19-1602.pdf), ACL 2019. -->
<!-- **Non-Autoregressive Text Generation** -->
<!-- 1. Min Liu‚Ä†, Yu Bao, Chengqi Zhao, Shujian Huang, [Selective Knowledge Distillation for Non-Autoregressive Neural Machine Translation](https://arxiv.org/abs/2303.17910), AAAI 2023.
2. Yu Bao, Hao Zhou, Shujian Huang, Dongqi Wang, Lihua Qian, Xinyu Dai, Jiajun Chen, Lei Li, [latent-GLAT: Glancing at Latent Variables for Parallel Text Generation](https://baoy-nlp.github.io/files/Latent_GLAT.pdf), ACL 2022.
3. Lihua Qian, Hao Zhou, Yu Bao, Mingxuan Wang, Lin Qiu, Weinan Zhang, Yong Yu, Lei Li, [Glancing Transformer for Non-Autoregressive Neural Machine Translation](https://aclanthology.org/2021.acl-long.155.pdf), ACL 2021.
4. Yu Bao, Shujian Huang, Tong Xiao, Dongqi Wang, Xinyu Dai, Jiajun Chen, [Non-Autoregressive Translation by Learning Target Categorical Codes](https://aclanthology.org/2021.naacl-main.458.pdf), NAACL-HLT 2021.
5. Yu Bao, Hao Zhou, Jiangtao Feng, Mingxuan Wang, Shujian Huang, Jiajun Chen, Lei Li, [PNAT: Non-Autoregressive Transformer by Position Learning](https://arxiv.org/abs/1911.10677), Preprint 2019. -->

<!-- ### Invited Talks

- Grammar Learning and Its Application for Molecular Design, Tsinghua University AIR, Oct. 2022.
- latent-GLAT: Glancing at Latent Variables for Parallel Text Generation, CIPSC & PaperWeekly & MLNLP, ACL-IJCAI-SIGIR, Apr. ‚Äî May. 2022.
- Research and Development of Parallel Text Generation, ByteDance AI Lab, Oct. 2021.
- Advice for Undergraduate Students, Northeast Forestry University, Nov. 2020. -->

## Professional Services

**<u>Area Chair of</u>**

- [ACL Rolling Review](https://aclrollingreview.org/reviewing) 2025-
- The 1st GenBio Workshop on New Frontiers of Generative AI and Biology at NeurIPS 2023

**<u>Journal Reviewer of</u>**

- IEEE Transaction on Pattern Analysis and Machine Intelligence (TPAMI)
- IEEE Transactions on Neural Networks and Learning Systems (TNNLS)
- Journal of Artificial Intelligence Research (JAIR)

**<u>PC Member/Reviewer of</u>**

- International Conference on Machine Learning (ICML) 2023-
- Annual Conference on Neural Information Processing Systems (NeurIPS) 2022-
- International Conference on Learning Representations(ICLR) 2022-
- North American Chapter of the Association for Computational Linguistics (NAACL) 2022-
- ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD) 2022-
- AAAI Conference on Artificial Intelligence (AAAI) 2022-
- Annual Meeting of the Association for Computational Linguistics (ACL) 2021-
- Conference on Empirical Methods in Natural Language Processing (EMNLP) 2021-
- The Chinese National Conference on Computational Linguistics (CCL) 2022
- The CCF Conference on Natural Language Processing and Chinese Computing (NLPCC) 2022
- International Joint Conferences on Artificial Intelligence (IJCAI) 2020

---

<center>
  <iframe src="https://calendar.google.com/calendar/embed?height=300&wkst=1&bgcolor=%23ffffff&ctz=Asia%2FShanghai&mode=AGENDA&showTabs=0&showTitle=1&showNav=0&showPrint=0&showTz=0&showCalendars=0&title=Schedule&src=d2VpZmVuZ2xpdXl1ZUBnbWFpbC5jb20&src=Z3IwY2l0a3NpMjQ5b3RhbGxuYWVjY2ZhamxlNmlkMm1AaW1wb3J0LmNhbGVuZGFyLmdvb2dsZS5jb20&color=%237986CB&color=%23E67C73" style="border-width:1" width="600" height="300" frameborder="0" scrolling="no">
  </iframe>
</center>

<center>
  <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=080808&w=353&t=tt&d=RXIJvl1M9HfHAiXc7AJe-qo0sHke2u_46ckL7Qp5HrY&co=ffffff&cmo=3acc3a&cmn=ff5353&ct=808080'></script>
</center>


<!--
**baoy-nlp/baoy-nlp** is a ‚ú® _special_ ‚ú® repository because its `README.md` (this file) appears on your GitHub profile.
Here are some ideas to get you started:

- üî≠ I‚Äôm currently working on ...
- üå± I‚Äôm currently learning ...
- üëØ I‚Äôm looking to collaborate on ...
- ü§î I‚Äôm looking for help with ...
- üí¨ Ask me about ...
- üì´ How to reach me: ...
- üòÑ Pronouns: ...
- ‚ö° Fun fact: ...
-->
